{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYNW1ENjK4oh2QvR393baS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LexanderThakur/LogisticRegression/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def initialize(n_features):\n",
        "  weights=np.zeros(n_features)\n",
        "  bias=0;\n",
        "  return weights,bias\n",
        "\n",
        "def propogate(X, y, weights, bias):\n",
        "    m = X.shape[0]\n",
        "    A = sigmoid(np.dot(X, weights) + bias)\n",
        "    epsilon = 1e-15\n",
        "    A = np.clip(A, epsilon, 1 - epsilon)\n",
        "\n",
        "    cost = (-1 / m) * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
        "    dw = (1 / m) * np.dot(X.T, (A - y))\n",
        "    db = (1 / m) * np.sum(A - y)\n",
        "\n",
        "    return dw, db, cost\n",
        "\n",
        "def train(X,y,lr=0.1,n_iterations=1000):\n",
        "  weights,bias=initialize(X.shape[1])\n",
        "\n",
        "  for i in range(0,1000):\n",
        "    dw,db,cost=propogate(X,y,weights,bias)\n",
        "    weights=weights-lr*dw\n",
        "    bias=bias-lr*db\n",
        "\n",
        "  return weights,bias\n",
        "\n",
        "\n",
        "def predict(X,weights,bias):\n",
        "  A=sigmoid(bias+np.dot(X,weights))\n",
        "\n",
        "  return (A>=0.5).astype(int)\n",
        "\n",
        "def accuracy(y_true,y_pred):\n",
        "\n",
        "  return np.mean(y_true==y_pred)\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X1, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X1)\n",
        "\n",
        "\n",
        "weights,bias=train(X,y)\n",
        "y_pred=predict(X,weights,bias)\n",
        "acc=accuracy(y,y_pred)\n",
        "print(f\"Training Accuracy= {acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFAUJAam94io",
        "outputId": "da95b158-de20-45d4-d4e3-fa76f24395a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy= 89.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass using soft max"
      ],
      "metadata": {
        "id": "hUk5HyuENfmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# 1. Softmax function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# 2. Initialize weights and bias\n",
        "def initialize(n_features, n_classes):\n",
        "    weights = np.zeros((n_features, n_classes))\n",
        "    bias = np.zeros((1, n_classes))\n",
        "    return weights, bias\n",
        "\n",
        "# 3. Forward and backward propagation\n",
        "def propagate(X, y_onehot, weights, bias):\n",
        "    m = X.shape[0]\n",
        "    z = np.dot(X, weights) + bias\n",
        "    A = softmax(z)\n",
        "\n",
        "    cost = (-1 / m) * np.sum(y_onehot * np.log(A + 1e-15))  # Add epsilon for stability\n",
        "\n",
        "    dw = (1 / m) * np.dot(X.T, (A - y_onehot))\n",
        "    db = (1 / m) * np.sum(A - y_onehot, axis=0, keepdims=True)\n",
        "\n",
        "    return dw, db, cost\n",
        "\n",
        "# 4. Training function\n",
        "def train(X, y, lr=0.1, n_iterations=1000):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = len(np.unique(y))\n",
        "\n",
        "    weights, bias = initialize(n_features, n_classes)\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        dw, db, cost = propagate(X, y_onehot, weights, bias)\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# 5. Predict function\n",
        "def predict(X, weights, bias):\n",
        "    z = np.dot(X, weights) + bias\n",
        "    A = softmax(z)\n",
        "    return np.argmax(A, axis=1)\n",
        "\n",
        "# 6. Accuracy function\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# 7. Generate multi-class data\n",
        "X_raw, y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=4,\n",
        "    n_informative=4,\n",
        "    n_redundant=0,\n",
        "    n_classes=3,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 8. Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_raw)\n",
        "\n",
        "# 9. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 10. Train the model\n",
        "weights, bias = train(X_train, y_train, lr=0.1, n_iterations=1000)\n",
        "\n",
        "# 11. Evaluate\n",
        "y_pred_train = predict(X_train, weights, bias)\n",
        "y_pred_test = predict(X_test, weights, bias)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracy(y_train, y_pred_train)*100:.2f}%\")\n",
        "print(f\"Test Accuracy: {accuracy(y_test, y_pred_test)*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "8yCCJOSBFj3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b34ea3-ee0d-41ae-ef6a-9c9db173ec5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 1.0986\n",
            "Iteration 100: Cost = 0.5921\n",
            "Iteration 200: Cost = 0.5698\n",
            "Iteration 300: Cost = 0.5652\n",
            "Iteration 400: Cost = 0.5638\n",
            "Iteration 500: Cost = 0.5632\n",
            "Iteration 600: Cost = 0.5629\n",
            "Iteration 700: Cost = 0.5628\n",
            "Iteration 800: Cost = 0.5628\n",
            "Iteration 900: Cost = 0.5627\n",
            "Train Accuracy: 76.25%\n",
            "Test Accuracy: 74.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using scikit learn\n"
      ],
      "metadata": {
        "id": "2Z14gYi7Rviy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=2,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_repeated=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test set accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIhkYuMZNlKi",
        "outputId": "4720e2d1-9c15-4712-9ccb-3fee3eb2dc54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 88.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FP7CT-qxSF-x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}